import asyncio
import datetime
import os
import re
import sys
import yaml
import shutil
from dotenv.main import load_dotenv
from telegram import Bot, InputFile
import jieba

if not os.getenv('TG_BOT_TOKEN') or not os.getenv('TG_CHAT_ID'):
    load_dotenv()

# å›ºå®šæ–‡æœ¬å¸¸é‡
WEEKLY_INTRO = ("æœ¬å‘¨åˆŠç”± PythonçŒ« å‡ºå“ï¼Œç²¾å¿ƒç­›é€‰å›½å†…å¤–çš„ 400+ ä¿¡æ¯æºï¼Œ"
                "ä¸ºä½ æŒ‘é€‰æœ€å€¼å¾—åˆ†äº«çš„æ–‡ç« ã€æ•™ç¨‹ã€å¼€æºé¡¹ç›®ã€è½¯ä»¶å·¥å…·ã€æ’­å®¢å’Œè§†é¢‘ã€çƒ­é—¨è¯é¢˜ç­‰å†…å®¹ã€‚"
                "æ„¿æ™¯ï¼šå¸®åŠ©æ‰€æœ‰è¯»è€…ç²¾è¿› Python æŠ€æœ¯ï¼Œå¹¶å¢žé•¿èŒä¸šå’Œå‰¯ä¸šçš„æ”¶å…¥ã€‚\n\n"
                "**æ¸©é¦¨æç¤ºï¼š** åœ¨å¾®ä¿¡å…³æ³¨ **PythonçŒ«**ï¼Œå‘é€æ•°å­—â€œ**9**â€ï¼Œå³å¯é¢†å– 9 æŠ˜ä¼˜æƒ ç ï¼Œè®¢é˜…ä¸“æ å¯äº« 15 å…ƒä¼˜æƒ ã€‚\n\n"
                "åŽ»ä¸“æ é˜…è¯»å…¨æ–‡ï¼š[å…¨æ–‡é“¾æŽ¥]()")

SUBSCRIPTION_INFO = ("å‘¨åˆŠå®žè¡Œä»˜è´¹è®¢é˜…åˆ¶ï¼Œå¹´è´¹ 148 å…ƒï¼Œå¹³å‡æ¯å¤© 4 æ¯›é’±ï¼Œä¸ºä½ ç²¾å‡†ç­›é€‰é«˜è´¨é‡æŠ€æœ¯å†…å®¹ã€‚"
                    "åœ¨ä¿¡æ¯æ´ªæµä¸­ä¸ºä½ æ·˜é‡‘ï¼ŒåŠ©åŠ›æŠ€æœ¯è§†é‡Žæ‹“å±•å’ŒèŒä¸šå‘å±•ï¼Œæ¬¢è¿Žè®¢é˜…ï¼š[https://xiaobot.net/p/python_weekly](https://xiaobot.net/p/python_weekly)")

SEASON2_SUMMARY = "[Python æ½®æµå‘¨åˆŠç¬¬3å­£æ€»ç»“ï¼Œé™„ç”µå­ä¹¦ä¸‹è½½](https://pythoncat.top/posts/2025-04-20-sweekly)"

FREE_COLLECTION = "[Python æ½®æµå‘¨åˆŠç¬¬äºŒå­£å®Œç»“ï¼ˆ31~60ï¼‰](https://pythoncat.top/posts/2025-04-20-iweekly)"

SEASON1_SUMMARY = "[Python æ½®æµå‘¨åˆŠç¬¬ä¸€å­£ç²¾åŽåˆé›†ï¼ˆ1~30ï¼‰](https://pythoncat.top/posts/2023-12-11-weekly)"

WECHAT_QR = "**å¾®ä¿¡å…³æ³¨ PythonçŒ«**ï¼š[https://img.pythoncat.top/python_cat.jpg](https://img.pythoncat.top/python_cat.jpg)"

FOOTER_SUBSCRIPTION = ("å‘¨åˆŠå®žè¡Œä»˜è´¹è®¢é˜…åˆ¶ï¼Œå¹´è´¹148å…ƒï¼Œé¢„è®¡50æœŸï¼Œè¶…è¿‡10ä¸‡å­—ã€‚çŽ°åœ¨è®¢é˜…ï¼Œæ¯å‘¨è®©è‡ªå·±è¿›æ­¥ä¸€ç‚¹ç‚¹ã€‚\n\n"
                      "ðŸ‘€ [å‰å¾€è®¢é˜…](https://weekly.pythoncat.top) \n\n"
                      "ðŸ‘€ [å…è´¹åˆé›†ä¸‹è½½](https://pythoncat.top/posts/2025-04-20-sweekly) \n\n")

# è‹±æ–‡ç‰ˆå›ºå®šæ–‡æœ¬å¸¸é‡
WEEKLY_INTRO_EN = ("Welcome to Python Trending Weekly - your gateway to cutting-edge Python intelligence! Curated by Python Cat from 400+ premium sources worldwide, "
                   "we deliver the most valuable articles, tutorials, open-source projects, tools, podcasts, videos, and trending discussions directly to your inbox. "
                   "Our mission: Accelerate your Python mastery and unlock new career opportunities in the ever-evolving tech landscape.\n\n"
                   "**Stay ahead of the curve:** [Subscribe now](https://www.patreon.com/pythonweekly) for weekly insights that keep you at the forefront of Python innovation!")

SUBSCRIPTION_INFO_EN = ("Cut through the noise with our premium subscription at $4.99/month. Get hand-picked, cutting-edge Python content delivered weekly. "
                        "Join 350+ professionals who trust us to filter the best from 400+ sources for technical vision expansion and career development. "
                        "Subscribe at: [Patreon](https://www.patreon.com/pythonweekly)")

SEASON2_SUMMARY_EN = "[Python Trending Weekly Season 3 Summary with E-book Download](https://pythoncat.top/posts/2025-04-20-sweekly)"

FREE_COLLECTION_EN = "[Python Trending Weekly Season 2 Complete Collection (Issues 31-60)](https://pythoncat.top/posts/2025-04-20-iweekly)"

SEASON1_SUMMARY_EN = "[Python Trending Weekly Season 1 Highlights Collection (Issues 1-30)](https://pythoncat.top/posts/2023-12-11-weekly)"

FOOTER_SUBSCRIPTION_EN = ("This newsletter operates on a paid subscription model at $20 per year, with an estimated 50 issues and over 100,000 words. "
                         "Subscribe now and make progress every week.\n\n"
                         "ðŸ‘€ [Patreon](https://www.patreon.com/pythonweekly) \n\n"
                         "ðŸ‘€ [Free Collection Download](https://pythoncat.top/posts/2025-04-20-sweekly) \n\n")

def split_and_generate_files(input_file, tmp_en_file):
    """
    åˆ†æ‹†æºæ–‡ä»¶çš„ä¸­è‹±æ–‡æ ‡é¢˜ï¼Œç”Ÿæˆä¸¤ä»½æ–‡ä»¶
    :param input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆä¸­è‹±åŒè¯­ç‰ˆï¼‰
    :param tmp_en_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆè‹±æ–‡ç‰ˆï¼‰
    æ³¨ï¼šä¸­æ–‡ç‰ˆä¼šè¦†ç›–åŽŸå§‹è¾“å…¥æ–‡ä»¶
    """
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # å®šä¹‰äºŒçº§æ ‡é¢˜çš„ä¸­è‹±æ–‡å¯¹ç…§è¡¨
    section_translations = {
        'ðŸ¦„æ–‡ç« &æ•™ç¨‹': 'ðŸ¦„Articles & Tutorials',
        'ðŸ¿ï¸é¡¹ç›®&èµ„æº': 'ðŸ¿ï¸Projects & Resources',
        'ðŸ¢æ’­å®¢&è§†é¢‘': 'ðŸ¢Podcasts & Videos',
        'ðŸ¥‚è®¨è®º&é—®é¢˜': 'ðŸ¥‚Discussions & Questions',
        'ðŸ§å¾€å¹´å›žé¡¾': 'ðŸ§Past Issues'
    }

    # æŸ¥æ‰¾æ‰€æœ‰markdowné“¾æŽ¥
    link_pattern = re.compile(r'\[([^\]]+)\]\(([^\)]+)\)')
    matches = link_pattern.findall(content)

    # åˆ†åˆ«å¤„ç†ä¸­è‹±æ–‡ç‰ˆæœ¬
    new_content1 = content  # ä¸­æ–‡ç‰ˆ
    new_content2 = content  # è‹±æ–‡ç‰ˆ
    
    for text, url in matches:
        if '---' in text:  # æ ‡é¢˜åŒ…å«ä¸­è‹±æ–‡åˆ†éš”ç¬¦
            parts = text.split('---', 1)
            new_text1 = parts[0]  # ä¸­æ–‡éƒ¨åˆ†
            new_text2 = parts[1] if len(parts) > 1 else ''  # è‹±æ–‡éƒ¨åˆ†
            new_content1 = new_content1.replace(f'[{text}]({url})', f'[{new_text1}]({url})')
            new_content2 = new_content2.replace(f'[{text}]({url})', f'[{new_text2}]({url})')
    
    # å¤„ç†äºŒçº§æ ‡é¢˜çš„ä¸­è‹±æ–‡è½¬æ¢
    for chinese_title, english_title in section_translations.items():
        # åŒ¹é…äºŒçº§æ ‡é¢˜æ ¼å¼: ## [ä¸­æ–‡æ ‡é¢˜](url)
        chinese_pattern = f'## \[{re.escape(chinese_title)}\]'
        english_replacement = f'## [{english_title}]'
        new_content2 = re.sub(chinese_pattern, english_replacement, new_content2)
        
        # åŒ¹é…ç²—ä½“æ ¼å¼: **[ä¸­æ–‡æ ‡é¢˜](url)**
        chinese_bold_pattern = f'\*\*\[{re.escape(chinese_title)}\]'
        english_bold_replacement = f'**[{english_title}]'
        new_content2 = re.sub(chinese_bold_pattern, english_bold_replacement, new_content2)

    # ä¿å­˜å¤„ç†åŽçš„æ–‡ä»¶
    with open(input_file, 'w', encoding='utf-8') as f1:
        f1.write(new_content1)
    
    with open(tmp_en_file, 'w', encoding='utf-8') as f2:
        f2.write(new_content2)

def read_md(file_path):
    """
    è¯»å–å¹¶è§£æžmarkdownæ–‡ä»¶ï¼Œè¿”å›žå†…å®¹äºŒçº§æ ‡é¢˜åŠå…¶å­æ ‡é¢˜
    :param file_path: markdownæ–‡ä»¶è·¯å¾„
    :return: å­—å…¸ï¼Œkeyä¸ºäºŒçº§æ ‡é¢˜ï¼Œvalueä¸ºå…¶ä¸‹çš„å­æ ‡é¢˜åˆ—è¡¨
    æ³¨ï¼šåªè¿”å›žåŒ…å«å­æ ‡é¢˜çš„éƒ¨åˆ†
    """
    with open(file_path, 'r', encoding="utf-8") as f:
        file_content = f.read()
        origin_content = parse_md(file_content)
        # è¿‡æ»¤æŽ‰æ²¡æœ‰å­æ ‡é¢˜çš„éƒ¨åˆ†
        new_content = {key: value for key, value in origin_content.items() if value}
        return new_content

def parse_md(file_content):
    """
    è§£æžmarkdownæ–‡ä»¶å†…å®¹ï¼Œæå–äºŒçº§æ ‡é¢˜å’Œå…¶ä¸‹çš„åˆ—è¡¨é¡¹
    :param file_content: markdownæ–‡ä»¶å†…å®¹
    :return: å­—å…¸ï¼Œkeyä¸ºäºŒçº§æ ‡é¢˜ï¼Œvalueä¸ºå…¶ä¸‹çš„åˆ—è¡¨é¡¹
    """
    # æå–æ‰€æœ‰äºŒçº§æ ‡é¢˜
    titles = re.findall(r'## (.*?)\n', file_content)
    # æå–äºŒçº§æ ‡é¢˜å’Œåˆ—è¡¨é¡¹ï¼ˆå¸¦é“¾æŽ¥çš„å½¢å¼ï¼‰
    sub_titles = re.findall(r'## (.*?)\n|\d+ã€\[(.*?)\]\(.*?\)', file_content)

    # åˆå§‹åŒ–ç»“æžœå­—å…¸
    parsed_content = {title: [] for title in titles}

    # éåŽ†åŒ¹é…ç»“æžœï¼Œå°†åˆ—è¡¨é¡¹æ·»åŠ åˆ°å¯¹åº”çš„æ ‡é¢˜ä¸‹
    current_title = None
    for title, sub_title in sub_titles:
        if title:  # æ‰¾åˆ°æ–°çš„äºŒçº§æ ‡é¢˜
            current_title = title
        elif current_title is not None and sub_title:  # æ‰¾åˆ°åˆ—è¡¨é¡¹
            parsed_content[current_title].append(sub_title.strip())

    return parsed_content

def content_to_string(contents):
    """
    å°†è§£æžåŽçš„markdownå†…å®¹è½¬æ¢ä¸ºæ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
    :param contents: åŒ…å«æ ‡é¢˜å’Œå­æ ‡é¢˜çš„å­—å…¸
    :return: æ ¼å¼åŒ–åŽçš„å­—ç¬¦ä¸²ï¼Œæ¯ä¸ªæ ‡é¢˜ä¸‹çš„åˆ—è¡¨é¡¹ä½¿ç”¨åœ†åœˆæ•°å­—æ ‡è®°
    """
    message = ""
    for section, sub_sections in contents.items():
        if sub_sections:  # åªå¤„ç†æœ‰å­æ ‡é¢˜çš„éƒ¨åˆ†
            message += f"**{section}**\n\n"
            for i, sub_section in enumerate(sub_sections, 1):
                message += f"{chr(9311 + i)} {sub_section}\n"
            message += "\n"
    return message

def get_front_matter(file_path):
    """
    è§£æžMarkdownæ–‡ä»¶çš„YAMLå…ƒæ•°æ®
    :param file_path: markdownæ–‡ä»¶è·¯å¾„
    :return: å…ƒæ•°æ®å­—å…¸
    """
    with open(file_path, 'r', encoding="utf-8") as f:
        file_content = f.read()
        # æå–YAMLå…ƒæ•°æ®éƒ¨åˆ†ï¼ˆä½äºŽæ–‡ä»¶å¼€å¤´çš„ä¸¤ä¸ª---ä¹‹é—´ï¼‰
        match = re.search(r'---\n(.*?)\n---', file_content, re.DOTALL)
        if match:
            return yaml.safe_load(match.group(1))

def write_summary_content(f, content_meta, md_body, weekly_no, source_file=None, with_metadata=False, is_english=False):
    """
    å†™å…¥æ‘˜è¦æ–‡ä»¶çš„å†…å®¹
    :param f: æ–‡ä»¶å¯¹è±¡
    :param content_meta: å…ƒæ•°æ®å­—å…¸
    :param md_body: æ­£æ–‡å†…å®¹
    :param weekly_no: æœŸå·
    :param source_file: æºæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºŽå¤åˆ¶å…ƒæ•°æ®
    :param with_metadata: æ˜¯å¦å†™å…¥å…ƒæ•°æ®
    :param is_english: æ˜¯å¦ä¸ºè‹±æ–‡ç‰ˆæœ¬
    """
    if with_metadata and source_file:
        # ä»Žæºæ–‡ä»¶å¤åˆ¶å®Œæ•´çš„å…ƒæ•°æ®
        with open(source_file, 'r', encoding='utf-8') as source:
            content = source.read()
            meta_match = re.search(r'---(.*?)---', content, re.DOTALL)
            if meta_match:
                f.write("---" + meta_match.group(1) + "---\n\n")
    else:
        # åªå†™å…¥æ ‡é¢˜
        f.write(f"# {content_meta['title']}\n\n")
    
    # æ ¹æ®è¯­è¨€ç‰ˆæœ¬é€‰æ‹©å¯¹åº”çš„æ–‡æœ¬å¸¸é‡
    if is_english:
        intro = WEEKLY_INTRO_EN
        subscription_info = SUBSCRIPTION_INFO_EN
        season2_summary = SEASON2_SUMMARY_EN
        free_collection = FREE_COLLECTION_EN
        season1_summary = SEASON1_SUMMARY_EN
        summary_text = "Here are the title summaries for this issue:"
        full_text_info = f"After subscribing, you can view the full text of Issue {weekly_no} for free:"
    else:
        intro = WEEKLY_INTRO
        subscription_info = SUBSCRIPTION_INFO
        season2_summary = SEASON2_SUMMARY
        free_collection = FREE_COLLECTION
        season1_summary = SEASON1_SUMMARY
        summary_text = "ä»¥ä¸‹æ˜¯æœ¬æœŸæ ‡é¢˜æ‘˜è¦ï¼š"
        full_text_info = f"è®¢é˜…åŽï¼Œå¯å…è´¹æŸ¥çœ‹ ç¬¬ {weekly_no} æœŸå‘¨åˆŠçš„å…¨æ–‡ï¼š"
    
    # å†™å…¥æ­£æ–‡å†…å®¹
    f.write(intro + "\n\n")
    f.write(f"{content_meta['description']}\n\n")
    f.write(f"{summary_text} \n\n")

    # æ·»åŠ æ¢è¡Œç¬¦ï¼Œè§£å†³æŸäº›å¹³å°æ— æ³•æ­£ç¡®æ¢è¡Œçš„é—®é¢˜
    formatted_body = md_body
    for i in range(1, 20):
        formatted_body = formatted_body.replace(chr(9311 + i), "\n" + chr(9311 + i))
    f.write(formatted_body + "\n\n")
    
    # å†™å…¥è®¢é˜…ä¿¡æ¯å’Œå…¶ä»–å›ºå®šå†…å®¹
    f.write(subscription_info + "\n\n")
    f.write(f"{full_text_info} \n\n")
    f.write(season2_summary + "\n\n")
    f.write(free_collection + "\n\n")
    f.write(season1_summary + "\n\n")
    
    # ä¸­æ–‡ç‰ˆæ‰æ˜¾ç¤ºå¾®ä¿¡äºŒç»´ç 
    if not is_english:
        f.write(WECHAT_QR + "\n\n")

def write_to_md_file(weekly_no, content_meta, md_body, pub_date, weekly_file):
    """
    ç”Ÿæˆä¸¤ä¸ªç‰ˆæœ¬çš„æ‘˜è¦æ–‡ä»¶ï¼š
    1. åšå®¢ç‰ˆæœ¬ï¼šä½¿ç”¨å®Œæ•´ç‰ˆçš„å…ƒæ•°æ®ï¼Œä½†å†…å®¹æ˜¯æ‘˜è¦
    2. Githubç‰ˆæœ¬ï¼šä»…ä¿ç•™titleå’ŒpubDateå…ƒæ•°æ®ï¼Œå†…å®¹æ˜¯æ‘˜è¦
    :param weekly_no: æœŸå·
    :param content_meta: å…ƒæ•°æ®å­—å…¸
    :param md_body: æ­£æ–‡å†…å®¹
    :param pub_date: å‘å¸ƒæ—¥æœŸ
    :param weekly_file: å‘¨åˆŠæ–‡ä»¶è·¯å¾„
    """
    # 1. ç”Ÿæˆåšå®¢ç‰ˆæœ¬ï¼ˆå®Œæ•´å…ƒæ•°æ®+æ‘˜è¦å†…å®¹ï¼‰
    blog_dir = os.path.expanduser('~/Documents/GitHub/astro-blog/src/pages/posts')
    if not os.path.exists(blog_dir):
        os.makedirs(blog_dir)
    
    blog_file = os.path.join(blog_dir, f"{pub_date}-weekly.md")
    if not os.path.exists(blog_file):
        print("Writing blog version summary...")
        with open(blog_file, 'w', encoding="utf-8") as f:
            write_summary_content(f, content_meta, md_body, weekly_no, weekly_file, True)
    
    # 2. ç”ŸæˆGithubç‰ˆæœ¬ï¼ˆç®€åŒ–å…ƒæ•°æ®+æ‘˜è¦å†…å®¹ï¼‰
    print("Writing github version summary...")
    with open(weekly_file, 'w', encoding='utf-8') as f:
        write_summary_content(f, content_meta, md_body, weekly_no)

def write_to_md_file_en(weekly_no, content_meta, md_body, pub_date, en_weekly_file):
    """
    ç”Ÿæˆè‹±æ–‡ç‰ˆæ‘˜è¦æ–‡ä»¶ï¼Œç”¨äºŽå‘å¸ƒåˆ°Mediumã€Dev.toç­‰è‹±æ–‡åšå®¢å¹³å°
    :param weekly_no: æœŸå·
    :param content_meta: å…ƒæ•°æ®å­—å…¸ï¼ˆæ¥è‡ªä¸­æ–‡ç‰ˆï¼Œä»…ç”¨äºŽæœŸå·ç­‰ä¿¡æ¯ï¼‰
    :param md_body: æ­£æ–‡å†…å®¹ï¼ˆæ¥è‡ªä¸­æ–‡ç‰ˆï¼Œä¸ä½¿ç”¨ï¼‰
    :param pub_date: å‘å¸ƒæ—¥æœŸ
    :param en_weekly_file: è‹±æ–‡å‘¨åˆŠæ–‡ä»¶è·¯å¾„
    """
    # ä»Žè‹±æ–‡ç‰ˆæ–‡ä»¶ä¸­è¯»å–çœŸæ­£çš„å†…å®¹å’Œå…ƒæ•°æ®
    if os.path.exists(en_weekly_file):
        en_content_meta = get_front_matter(en_weekly_file)
        en_content_body = content_to_string(read_md(en_weekly_file))
    else:
        print(f"Warning: English weekly file not found: {en_weekly_file}")
        return None
    
    # ç”Ÿæˆè‹±æ–‡ç‰ˆæ‘˜è¦æ–‡ä»¶
    en_summary_dir = 'docs/en'
    if not os.path.exists(en_summary_dir):
        os.makedirs(en_summary_dir)
    
    en_summary_file = os.path.join(en_summary_dir, f"{pub_date}-weekly.md")
    print("Writing English version summary...")
    with open(en_summary_file, 'w', encoding='utf-8') as f:
        write_summary_content(f, en_content_meta, en_content_body, weekly_no, is_english=True)
    
    return en_summary_file

def set_title(no):
    """
    ç”ŸæˆTelegramæ¶ˆæ¯çš„æ ‡é¢˜éƒ¨åˆ†
    :param no: æœŸå·
    :return: æ ¼å¼åŒ–çš„æ ‡é¢˜å­—ç¬¦ä¸²
    """
    tag = "#Pythonæ½®æµå‘¨åˆŠ \n\n"
    title = f"ðŸ±ðŸ±ðŸ±ðŸ±  ç¬¬ {no} æœŸ  ðŸ±ðŸ±ðŸ±ðŸ±\n\n"
    return tag + title

def set_content_body(file_path, weekly_no, pub_date):
    """
    ç”Ÿæˆå¹¶è¿”å›žæ¶ˆæ¯çš„æ­£æ–‡éƒ¨åˆ†ï¼ŒåŒæ—¶ç”Ÿæˆæ‘˜è¦æ–‡ä»¶
    :param file_path: å‘¨åˆŠæ–‡ä»¶è·¯å¾„
    :param weekly_no: æœŸå·
    :param pub_date: å‘å¸ƒæ—¥æœŸ
    :return: æ ¼å¼åŒ–çš„æ­£æ–‡å†…å®¹
    """
    content_meta = get_front_matter(file_path)
    content_body = content_to_string(read_md(file_path))
    # åœ¨éžåœ¨çº¿çŽ¯å¢ƒä¸‹ç”Ÿæˆæ‘˜è¦æ–‡ä»¶
    online_action = os.getenv('ONLINE_ACTION')
    if not online_action:
        write_to_md_file(weekly_no, content_meta, content_body, pub_date, file_path)
    return content_body

def set_footer():
    """
    è¿”å›žTelegramæ¶ˆæ¯çš„é¡µè„šéƒ¨åˆ†
    :return: è®¢é˜…ä¿¡æ¯æ–‡æœ¬
    """
    return FOOTER_SUBSCRIPTION

def set_channel():
    """
    è¿”å›žTelegramé¢‘é“ä¿¡æ¯
    :return: é¢‘é“ä¿¡æ¯æ–‡æœ¬
    """
    return "ðŸ±é¢‘é“ @pythontrendingweekly"

async def send_to_telegram(bot_token, chat_id, text, image_path=None):
    """
    å‘é€æ¶ˆæ¯åˆ°Telegramé¢‘é“
    :param bot_token: Telegramæœºå™¨äººtoken
    :param chat_id: ç›®æ ‡é¢‘é“ID
    :param text: æ¶ˆæ¯æ–‡æœ¬
    :param image_path: å¯é€‰çš„å›¾ç‰‡è·¯å¾„
    """
    print("Sending content to tg bot")
    bot = Bot(token=bot_token)
    if image_path:
        with open(image_path, 'rb') as f:
            await bot.send_photo(chat_id=chat_id, photo=InputFile(f), caption=text, parse_mode='Markdown')
    else:
        await bot.send_message(chat_id=chat_id, text=text, parse_mode='Markdown', disable_web_page_preview=True)

def extract_weekly_no(file_path):
    """
    ä»Žæ–‡ä»¶å†…å®¹ä¸­æå–æœŸå·
    :param file_path: å‘¨åˆŠæ–‡ä»¶è·¯å¾„
    :return: æœŸå·å­—ç¬¦ä¸²
    """
    print(f"Extracting weekly number from {file_path}")
    with open(file_path, 'r', encoding="utf-8") as f:
        lines = f.readlines()
        match = re.search(r'#(\d+)', lines[2])
        if match:
            return match.group(1)
        else:
            raise ValueError("Invalid weekly no format in the third line.")

def get_message(weekly_no, content_body):
    """
    ç»„è£…å®Œæ•´çš„Telegramæ¶ˆæ¯
    :param weekly_no: æœŸå·
    :param content_body: æ­£æ–‡å†…å®¹
    :return: å®Œæ•´çš„æ¶ˆæ¯æ–‡æœ¬
    """
    print("Getting weekly message")
    header = set_title(weekly_no)
    footer = set_footer()
    channel = set_channel()
    return header + content_body + footer + channel

def count_words(file_path):
    """
    ç»Ÿè®¡markdownæ–‡ä»¶çš„å­—æ•°
    :param file_path: æ–‡ä»¶è·¯å¾„
    :return: å­—æ•°ç»Ÿè®¡
    æ³¨ï¼šä¸åŒ…æ‹¬é“¾æŽ¥URLå’Œå…ƒæ•°æ®éƒ¨åˆ†
    """
    print("Counting words in the file...")
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ç§»é™¤markdownçš„å…ƒæ•°æ®
    content = re.sub(r'---.*?---', '', content, flags=re.DOTALL)
    
    # ä¿ç•™é“¾æŽ¥æ–‡æœ¬ï¼Œç§»é™¤URL
    content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)
    
    # ç§»é™¤å…¶ä»–markdownæ ‡è®°
    content = re.sub(r'[#*`>]', '', content)  # ç§»é™¤#ã€*ã€`ã€>ç­‰markdownæ ‡è®°
    content = re.sub(r'\n+', '\n', content)   # å°†å¤šä¸ªæ¢è¡Œç¬¦æ›¿æ¢ä¸ºå•ä¸ª
    
    # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
    words = jieba.lcut(content)
    # è¿‡æ»¤æŽ‰ç©ºç™½å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·
    words = [word for word in words if word.strip()]
    
    return len(words)

def update_word_count(file_path, word_count):
    """
    æ›´æ–°æ–‡ä»¶ä¸­çš„å­—æ•°ç»Ÿè®¡
    :param file_path: æ–‡ä»¶è·¯å¾„
    :param word_count: å­—æ•°ç»Ÿè®¡ç»“æžœ
    """
    print("Updating word count in the file...")
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢å­—æ•°ï¼ŒåŒæ—¶å¤„ç†æœ‰æ•°å­—å’Œæ— æ•°å­—çš„æƒ…å†µ
    updated_content = re.sub(r'å…¨æ–‡(\s*?)(\d+)?(\s*?)å­—', f'å…¨æ–‡ {word_count} å­—', content)
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(updated_content)

def copy_to_archive(source_file, pub_date, weekly_no):
    """
    å¤åˆ¶ä¸­æ–‡å®Œæ•´ç‰ˆåˆ°é¡¹ç›®å½’æ¡£ç›®å½•
    :param source_file: æºæ–‡ä»¶è·¯å¾„
    :param pub_date: å‘å¸ƒæ—¥æœŸ
    :param weekly_no: æœŸå·
    """
    # èŽ·å–é¡¹ç›®æ ¹ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    ebook_dir = os.path.join(project_root, 'docs', 'tmp')
    if not os.path.exists(ebook_dir):
        os.makedirs(ebook_dir)
    ebook_target = os.path.join(ebook_dir, f'{pub_date}-weekly.md')
    print(f"Copying Chinese version to project archive directory: {ebook_target}")
    shutil.copy2(source_file, ebook_target)

def update_readme(weekly_file, weekly_no):
    """
    æ›´æ–°READMEæ–‡ä»¶ï¼Œåœ¨å¾€æœŸåˆ—è¡¨éƒ¨åˆ†æ·»åŠ æ–°çš„å‘¨åˆŠé“¾æŽ¥
    :param weekly_file: å‘¨åˆŠæ–‡ä»¶è·¯å¾„
    :param weekly_no: æœŸå·
    """
    print("Updating README files...")
    
    # è¯»å–å‘¨åˆŠæ–‡ä»¶çš„å…ƒæ•°æ®
    content_meta = get_front_matter(weekly_file)
    if not content_meta:
        print("Warning: Could not find front matter in weekly file")
        return
    
    # ä»Žtitleä¸­æå–å®žé™…æ ‡é¢˜ï¼ˆå†’å·åŽçš„å†…å®¹ï¼‰
    title = content_meta['title'].split('ï¼š', 1)[1] if 'ï¼š' in content_meta['title'] else content_meta['title']
    
    # æ›´æ–°ä¸­æ–‡README
    update_single_readme('README_ZH.md', weekly_file, weekly_no, title, content_meta['description'], 
                        "## ðŸ¦„å¾€æœŸåˆ—è¡¨\n\n", f"- ç¬¬ {weekly_no} æœŸï¼š[{title}](./{weekly_file})\n")
    
    # æ›´æ–°è‹±æ–‡README  
    update_single_readme('README.md', weekly_file, weekly_no, title, content_meta['description'],
                        "## ðŸ¦„ Past Issues\n\n", f"- Issue {weekly_no}: [{title}](./{weekly_file})\n")

def update_single_readme(readme_file, weekly_file, weekly_no, title, description, section_start, entry_format):
    """
    æ›´æ–°å•ä¸ªREADMEæ–‡ä»¶
    :param readme_file: READMEæ–‡ä»¶è·¯å¾„
    :param weekly_file: å‘¨åˆŠæ–‡ä»¶è·¯å¾„
    :param weekly_no: æœŸå·
    :param title: æ ‡é¢˜
    :param description: æè¿°
    :param section_start: å¾€æœŸåˆ—è¡¨éƒ¨åˆ†çš„å¼€å§‹æ ‡è®°
    :param entry_format: æ¡ç›®æ ¼å¼
    """
    try:
        # è¯»å–READMEæ–‡ä»¶
        with open(readme_file, 'r', encoding='utf-8') as f:
            readme_content = f.read()
        
        # ç”Ÿæˆæ–°çš„å‘¨åˆŠæ¡ç›®
        new_entry = entry_format
        new_entry += f"  - {description}\n"
        
        # åœ¨å¾€æœŸåˆ—è¡¨éƒ¨åˆ†æ’å…¥æ–°æ¡ç›®
        if section_start in readme_content:
            parts = readme_content.split(section_start)
            # åœ¨å¾€æœŸåˆ—è¡¨çš„å¼€å¤´æ’å…¥æ–°æ¡ç›®
            updated_content = parts[0] + section_start + new_entry + parts[1]
            
            # å†™å…¥æ›´æ–°åŽçš„å†…å®¹
            with open(readme_file, 'w', encoding='utf-8') as f:
                f.write(updated_content)
                print(f"{readme_file} updated successfully!")
        else:
            print(f"Warning: Could not find past issues section in {readme_file}")
    except FileNotFoundError:
        print(f"Warning: {readme_file} not found")
    except Exception as e:
        print(f"Error updating {readme_file}: {e}")

def process_weekly(pub_date=None):
    """
    å¤„ç†å‘¨åˆŠçš„ä¸»å‡½æ•°
    :param pub_date: å¯é€‰çš„å‘å¸ƒæ—¥æœŸï¼Œé»˜è®¤ä¸ºå½“å¤©
    å¤„ç†æµç¨‹ï¼š
    1. æ›´æ–°README.mdæ–‡ä»¶ï¼ˆä½¿ç”¨åŽŸå§‹çš„ä¸­è‹±å®Œæ•´ç‰ˆï¼‰
    2. æ‹†åˆ†ä¸­è‹±æ–‡ç‰ˆæœ¬ï¼ˆè‹±æ–‡ç‰ˆè‡ªåŠ¨ä¿å­˜åˆ°tmpç›®å½•ï¼Œä¸­æ–‡ç‰ˆè¦†ç›–åŽŸæ–‡ä»¶ï¼‰
    3. ç»Ÿè®¡ä¸­æ–‡ç‰ˆå­—æ•°å¹¶æ›´æ–°
    4. å¤åˆ¶ä¸­æ–‡ç‰ˆåˆ°ebookå½’æ¡£ç›®å½•
    5. ç”Ÿæˆä¸­æ–‡ç‰ˆæ‘˜è¦æ–‡ä»¶ï¼ˆåšå®¢ç‰ˆå’ŒGithubç‰ˆï¼‰
    6. ç”Ÿæˆè‹±æ–‡ç‰ˆæ‘˜è¦æ–‡ä»¶ï¼ˆç”¨äºŽMediumã€Dev.toç­‰å¹³å°ï¼‰
    7. å‘é€æ¶ˆæ¯ç‰ˆåˆ°Telegram
    """
    if pub_date is None:
        pub_date = datetime.datetime.now().strftime('%Y-%m-%d')
    
    weekly_file = f'docs/{pub_date}-weekly.md'
    tmp_en_file = f'docs/en/tmp/{pub_date}-weekly.md'
    
    if not os.path.exists(weekly_file):
        print(f"File {weekly_file} does not exist.")
        sys.exit(1)
    
    # 1. æ›´æ–°README
    print("1. Updating README.md...")
    weekly_no = extract_weekly_no(weekly_file)
    update_readme(weekly_file, weekly_no)
    
    # 2. æ‹†åˆ†ä¸­è‹±æ–‡ç‰ˆæœ¬
    print("2. Splitting Chinese and English versions...")
    split_and_generate_files(weekly_file, tmp_en_file)
    
    # 3. ç»Ÿè®¡ä¸­æ–‡ç‰ˆå­—æ•°å¹¶æ›´æ–°
    print("3. Counting words and updating...")
    word_count = count_words(weekly_file)
    update_word_count(weekly_file, word_count)
    
    # 4. å¤åˆ¶å®Œæ•´ç‰ˆæ–‡ä»¶åˆ°å½’æ¡£ç›®å½•
    print("4. Copying files to archive...")
    copy_to_archive(weekly_file, pub_date, weekly_no)
    
    # 5. ç”Ÿæˆä¸­æ–‡ç‰ˆå‘¨åˆŠæ‘˜è¦
    print("5. Generating Chinese summary files...")
    content_meta = get_front_matter(weekly_file)
    content_body = content_to_string(read_md(weekly_file))
    write_to_md_file(weekly_no, content_meta, content_body, pub_date, weekly_file)
    
    # 6. ç”Ÿæˆè‹±æ–‡ç‰ˆå‘¨åˆŠæ‘˜è¦ï¼ˆä»…åšå®¢ç‰ˆï¼‰
    print("6. Generating English blog summary...")
    if os.path.exists(tmp_en_file):
        en_content_meta = get_front_matter(tmp_en_file)
        en_content_body = content_to_string(read_md(tmp_en_file))
        en_summary_file = write_to_md_file_en(weekly_no, en_content_meta, en_content_body, pub_date, tmp_en_file)
        print(f"English blog summary generated: {en_summary_file}")
    else:
        print("Warning: English version file not found, skipping English summary generation.")
    
    # 7. ç”Ÿæˆä¸­æ–‡ç‰ˆæ‘˜è¦æ¶ˆæ¯å¹¶å‘é€åˆ°Telegram
    print("7. Generating Chinese summary and sending to Telegram...")
    message = get_message(weekly_no, content_body)
    tg_bot_token = os.environ['TG_BOT_TOKEN']
    tg_chat_id = os.environ['TG_CHAT_ID']
    image_path = "resources/img/python-weekly.jpg"
    asyncio.run(send_to_telegram(tg_bot_token, tg_chat_id, message, image_path))
    
    print("Weekly processing completed!")
    print("Files generated:")
    print(f"  - Chinese version: {weekly_file}")
    print(f"  - English version: {tmp_en_file}")
    if os.path.exists(tmp_en_file):
        print(f"  - English summary: docs/en/summary/{pub_date}-weekly-summary.md")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        process_weekly(sys.argv[1])
    else:
        process_weekly()