import asyncio
import datetime
import os
import re
import sys
import yaml
import shutil
from dotenv.main import load_dotenv
from telegram import Bot, InputFile
import jieba

if not os.getenv('TG_BOT_TOKEN') or not os.getenv('TG_CHAT_ID'):
    load_dotenv()

# å›ºå®šæ–‡æœ¬å¸¸é‡
WEEKLY_INTRO = ("æœ¬å‘¨åˆŠç”± PythonçŒ« å‡ºå“ï¼Œç²¾å¿ƒç­›é€‰å›½å†…å¤–çš„ 250+ ä¿¡æ¯æºï¼Œ"
                "ä¸ºä½ æŒ‘é€‰æœ€å€¼å¾—åˆ†äº«çš„æ–‡ç« ã€æ•™ç¨‹ã€å¼€æºé¡¹ç›®ã€è½¯ä»¶å·¥å…·ã€æ’­å®¢å’Œè§†é¢‘ã€çƒ­é—¨è¯é¢˜ç­‰å†…å®¹ã€‚"
                "æ„¿æ™¯ï¼šå¸®åŠ©æ‰€æœ‰è¯»è€…ç²¾è¿› Python æŠ€æœ¯ï¼Œå¹¶å¢žé•¿èŒä¸šå’Œå‰¯ä¸šçš„æ”¶å…¥ã€‚\n\n"
                "**æ¸©é¦¨æç¤ºï¼š** åœ¨å¾®ä¿¡å…³æ³¨ **PythonçŒ«**ï¼Œå‘é€â€œ**ä¼˜æƒ åˆ¸**â€ï¼Œå³å¯é¢†å– 9 æŠ˜ä¼˜æƒ ç ï¼Œè®¢é˜…ä¸“æ å¯äº« 15 å…ƒä¼˜æƒ ã€‚")

SUBSCRIPTION_INFO = ("å‘¨åˆŠå®žè¡Œä»˜è´¹è®¢é˜…åˆ¶ï¼Œå¹´è´¹ 148 å…ƒï¼Œå¹³å‡æ¯å¤© 4 æ¯›é’±ï¼Œç»å¯¹æ˜¯ä¸€ç¬”æœ‰çœ¼å…‰çš„æŠ•èµ„ã€‚"
                    "èŠ±é’±å­¦ä¹ çŸ¥è¯†ï¼ŒèŠ±é’±æå‡è‡ªå·±ï¼Œæ¬¢è¿Žè®¢é˜…è¿™ä¸ªä½ ç»å¯¹ä¸ä¼šåŽæ‚”çš„ä¸“æ ï¼š[https://xiaobot.net/p/python_weekly](https://xiaobot.net/p/python_weekly)")

SEASON2_SUMMARY = "[Python æ½®æµå‘¨åˆŠç¬¬3å­£æ€»ç»“ï¼Œé™„ç”µå­ä¹¦ä¸‹è½½](https://pythoncat.top/posts/2025-04-20-sweekly)"

FREE_COLLECTION = "[Python æ½®æµå‘¨åˆŠç¬¬äºŒå­£å®Œç»“ï¼ˆ31~60ï¼‰](https://pythoncat.top/posts/2025-04-20-iweekly)"

SEASON1_SUMMARY = "[Python æ½®æµå‘¨åˆŠç¬¬ä¸€å­£ç²¾åŽåˆé›†ï¼ˆ1~30ï¼‰](https://pythoncat.top/posts/2023-12-11-weekly)"

WECHAT_QR = "**å¾®ä¿¡å…³æ³¨ PythonçŒ«**ï¼š[https://img.pythoncat.top/python_cat.jpg](https://img.pythoncat.top/python_cat.jpg)"

FOOTER_SUBSCRIPTION = ("å‘¨åˆŠå®žè¡Œä»˜è´¹è®¢é˜…åˆ¶ï¼Œå¹´è´¹148å…ƒï¼Œé¢„è®¡50æœŸï¼Œè¶…è¿‡10ä¸‡å­—ã€‚çŽ°åœ¨è®¢é˜…ï¼Œæ¯å‘¨è®©è‡ªå·±è¿›æ­¥ä¸€ç‚¹ç‚¹ã€‚\n\n"
                      "ðŸ‘€ [è®¢é˜…æ–¹å¼ä¸€ï¼ˆå°æŠ¥ç«¥ï¼‰](https://xiaobot.net/p/python_weekly) \n\n"
                      "ðŸ‘€ [è®¢é˜…æ–¹å¼äºŒï¼ˆçˆ±å‘ç”µï¼‰](https://afdian.com/a/python_weekly) \n\n"
                      "ðŸ‘€ [å…è´¹åˆé›†ä¸‹è½½](https://pythoncat.top/posts/2025-04-20-sweekly) \n\n")

def split_and_generate_files(input_file, tmp_en_file):
    """
    åˆ†æ‹†æºæ–‡ä»¶çš„ä¸­è‹±æ–‡æ ‡é¢˜ï¼Œç”Ÿæˆä¸¤ä»½æ–‡ä»¶
    :param input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆä¸­è‹±åŒè¯­ç‰ˆï¼‰
    :param tmp_en_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆè‹±æ–‡ç‰ˆï¼‰
    æ³¨ï¼šä¸­æ–‡ç‰ˆä¼šè¦†ç›–åŽŸå§‹è¾“å…¥æ–‡ä»¶
    """
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # æŸ¥æ‰¾æ‰€æœ‰markdowné“¾æŽ¥
    link_pattern = re.compile(r'\[([^\]]+)\]\(([^\)]+)\)')
    matches = link_pattern.findall(content)

    # åˆ†åˆ«å¤„ç†ä¸­è‹±æ–‡ç‰ˆæœ¬
    new_content1 = content  # ä¸­æ–‡ç‰ˆ
    new_content2 = content  # è‹±æ–‡ç‰ˆ
    for text, url in matches:
        if '---' in text:  # æ ‡é¢˜åŒ…å«ä¸­è‹±æ–‡åˆ†éš”ç¬¦
            parts = text.split('---', 1)
            new_text1 = parts[0]  # ä¸­æ–‡éƒ¨åˆ†
            new_text2 = parts[1] if len(parts) > 1 else ''  # è‹±æ–‡éƒ¨åˆ†
            new_content1 = new_content1.replace(f'[{text}]({url})', f'[{new_text1}]({url})')
            new_content2 = new_content2.replace(f'[{text}]({url})', f'[{new_text2}]({url})')

    # ä¿å­˜å¤„ç†åŽçš„æ–‡ä»¶
    with open(input_file, 'w', encoding='utf-8') as f1:
        f1.write(new_content1)
    
    with open(tmp_en_file, 'w', encoding='utf-8') as f2:
        f2.write(new_content2)

def read_md(file_path):
    """
    è¯»å–å¹¶è§£æžmarkdownæ–‡ä»¶ï¼Œè¿”å›žå†…å®¹äºŒçº§æ ‡é¢˜åŠå…¶å­æ ‡é¢˜
    :param file_path: markdownæ–‡ä»¶è·¯å¾„
    :return: å­—å…¸ï¼Œkeyä¸ºäºŒçº§æ ‡é¢˜ï¼Œvalueä¸ºå…¶ä¸‹çš„å­æ ‡é¢˜åˆ—è¡¨
    æ³¨ï¼šåªè¿”å›žåŒ…å«å­æ ‡é¢˜çš„éƒ¨åˆ†
    """
    with open(file_path, 'r', encoding="utf-8") as f:
        file_content = f.read()
        origin_content = parse_md(file_content)
        # è¿‡æ»¤æŽ‰æ²¡æœ‰å­æ ‡é¢˜çš„éƒ¨åˆ†
        new_content = {key: value for key, value in origin_content.items() if value}
        return new_content

def parse_md(file_content):
    """
    è§£æžmarkdownæ–‡ä»¶å†…å®¹ï¼Œæå–äºŒçº§æ ‡é¢˜å’Œå…¶ä¸‹çš„åˆ—è¡¨é¡¹
    :param file_content: markdownæ–‡ä»¶å†…å®¹
    :return: å­—å…¸ï¼Œkeyä¸ºäºŒçº§æ ‡é¢˜ï¼Œvalueä¸ºå…¶ä¸‹çš„åˆ—è¡¨é¡¹
    """
    # æå–æ‰€æœ‰äºŒçº§æ ‡é¢˜
    titles = re.findall(r'## (.*?)\n', file_content)
    # æå–äºŒçº§æ ‡é¢˜å’Œåˆ—è¡¨é¡¹ï¼ˆå¸¦é“¾æŽ¥çš„å½¢å¼ï¼‰
    sub_titles = re.findall(r'## (.*?)\n|\d+ã€\[(.*?)\]\(.*?\)', file_content)

    # åˆå§‹åŒ–ç»“æžœå­—å…¸
    parsed_content = {title: [] for title in titles}

    # éåŽ†åŒ¹é…ç»“æžœï¼Œå°†åˆ—è¡¨é¡¹æ·»åŠ åˆ°å¯¹åº”çš„æ ‡é¢˜ä¸‹
    current_title = None
    for title, sub_title in sub_titles:
        if title:  # æ‰¾åˆ°æ–°çš„äºŒçº§æ ‡é¢˜
            current_title = title
        elif current_title is not None and sub_title:  # æ‰¾åˆ°åˆ—è¡¨é¡¹
            parsed_content[current_title].append(sub_title.strip())

    return parsed_content

def content_to_string(contents):
    """
    å°†è§£æžåŽçš„markdownå†…å®¹è½¬æ¢ä¸ºæ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
    :param contents: åŒ…å«æ ‡é¢˜å’Œå­æ ‡é¢˜çš„å­—å…¸
    :return: æ ¼å¼åŒ–åŽçš„å­—ç¬¦ä¸²ï¼Œæ¯ä¸ªæ ‡é¢˜ä¸‹çš„åˆ—è¡¨é¡¹ä½¿ç”¨åœ†åœˆæ•°å­—æ ‡è®°
    """
    message = ""
    for section, sub_sections in contents.items():
        if sub_sections:  # åªå¤„ç†æœ‰å­æ ‡é¢˜çš„éƒ¨åˆ†
            message += f"**{section}**\n\n"
            for i, sub_section in enumerate(sub_sections, 1):
                message += f"{chr(9311 + i)} {sub_section}\n"
            message += "\n"
    return message

def get_front_matter(file_path):
    """
    è§£æžMarkdownæ–‡ä»¶çš„YAMLå…ƒæ•°æ®
    :param file_path: markdownæ–‡ä»¶è·¯å¾„
    :return: å…ƒæ•°æ®å­—å…¸
    """
    with open(file_path, 'r', encoding="utf-8") as f:
        file_content = f.read()
        # æå–YAMLå…ƒæ•°æ®éƒ¨åˆ†ï¼ˆä½äºŽæ–‡ä»¶å¼€å¤´çš„ä¸¤ä¸ª---ä¹‹é—´ï¼‰
        match = re.search(r'---\n(.*?)\n---', file_content, re.DOTALL)
        if match:
            return yaml.safe_load(match.group(1))

def write_summary_content(f, content_meta, md_body, weekly_no, source_file=None, with_metadata=False):
    """
    å†™å…¥æ‘˜è¦æ–‡ä»¶çš„å†…å®¹
    :param f: æ–‡ä»¶å¯¹è±¡
    :param content_meta: å…ƒæ•°æ®å­—å…¸
    :param md_body: æ­£æ–‡å†…å®¹
    :param weekly_no: æœŸå·
    :param source_file: æºæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºŽå¤åˆ¶å…ƒæ•°æ®
    :param with_metadata: æ˜¯å¦å†™å…¥å…ƒæ•°æ®
    """
    if with_metadata and source_file:
        # ä»Žæºæ–‡ä»¶å¤åˆ¶å®Œæ•´çš„å…ƒæ•°æ®
        with open(source_file, 'r', encoding='utf-8') as source:
            content = source.read()
            meta_match = re.search(r'---(.*?)---', content, re.DOTALL)
            if meta_match:
                f.write("---" + meta_match.group(1) + "---\n\n")
    else:
        # åªå†™å…¥æ ‡é¢˜
        f.write(f"# {content_meta['title']}\n\n")
    
    # å†™å…¥æ­£æ–‡å†…å®¹
    f.write(WEEKLY_INTRO + "\n\n")
    f.write(f"{content_meta['description']}\n\n")
    f.write("ä»¥ä¸‹æ˜¯æœ¬æœŸæ‘˜è¦ï¼š \n\n")

    # æ·»åŠ æ¢è¡Œç¬¦ï¼Œè§£å†³æŸäº›å¹³å°æ— æ³•æ­£ç¡®æ¢è¡Œçš„é—®é¢˜
    formatted_body = md_body
    for i in range(1, 20):
        formatted_body = formatted_body.replace(chr(9311 + i), "\n" + chr(9311 + i))
    f.write(formatted_body + "\n\n")
    
    # å†™å…¥è®¢é˜…ä¿¡æ¯å’Œå…¶ä»–å›ºå®šå†…å®¹
    f.write(SUBSCRIPTION_INFO + "\n\n")
    f.write(f"è®¢é˜…åŽï¼Œå¯å…è´¹æŸ¥çœ‹ ç¬¬ {weekly_no} æœŸå‘¨åˆŠçš„å…¨æ–‡ï¼š \n\n")
    f.write(SEASON2_SUMMARY + "\n\n")
    f.write(FREE_COLLECTION + "\n\n")
    f.write(SEASON1_SUMMARY + "\n\n")
    f.write(WECHAT_QR + "\n\n")

def write_to_md_file(weekly_no, content_meta, md_body, pub_date, weekly_file):
    """
    ç”Ÿæˆä¸¤ä¸ªç‰ˆæœ¬çš„æ‘˜è¦æ–‡ä»¶ï¼š
    1. åšå®¢ç‰ˆæœ¬ï¼šä½¿ç”¨å®Œæ•´ç‰ˆçš„å…ƒæ•°æ®ï¼Œä½†å†…å®¹æ˜¯æ‘˜è¦
    2. Githubç‰ˆæœ¬ï¼šä»…ä¿ç•™titleå’ŒpubDateå…ƒæ•°æ®ï¼Œå†…å®¹æ˜¯æ‘˜è¦
    :param weekly_no: æœŸå·
    :param content_meta: å…ƒæ•°æ®å­—å…¸
    :param md_body: æ­£æ–‡å†…å®¹
    :param pub_date: å‘å¸ƒæ—¥æœŸ
    :param weekly_file: å‘¨åˆŠæ–‡ä»¶è·¯å¾„
    """
    # 1. ç”Ÿæˆåšå®¢ç‰ˆæœ¬ï¼ˆå®Œæ•´å…ƒæ•°æ®+æ‘˜è¦å†…å®¹ï¼‰
    blog_dir = os.path.expanduser('~/Documents/GitHub/astro-blog/src/pages/posts')
    if not os.path.exists(blog_dir):
        os.makedirs(blog_dir)
    
    blog_file = os.path.join(blog_dir, f"{pub_date}-weekly.md")
    if not os.path.exists(blog_file):
        print("Writing blog version summary...")
        with open(blog_file, 'w', encoding="utf-8") as f:
            write_summary_content(f, content_meta, md_body, weekly_no, weekly_file, True)
    
    # 2. ç”ŸæˆGithubç‰ˆæœ¬ï¼ˆç®€åŒ–å…ƒæ•°æ®+æ‘˜è¦å†…å®¹ï¼‰
    print("Writing github version summary...")
    with open(weekly_file, 'w', encoding='utf-8') as f:
        write_summary_content(f, content_meta, md_body, weekly_no)

def set_title(no):
    """
    ç”ŸæˆTelegramæ¶ˆæ¯çš„æ ‡é¢˜éƒ¨åˆ†
    :param no: æœŸå·
    :return: æ ¼å¼åŒ–çš„æ ‡é¢˜å­—ç¬¦ä¸²
    """
    tag = "#Pythonæ½®æµå‘¨åˆŠ \n\n"
    title = f"ðŸ±ðŸ±ðŸ±ðŸ±  ç¬¬ {no} æœŸ  ðŸ±ðŸ±ðŸ±ðŸ±\n\n"
    return tag + title

def set_content_body(file_path, weekly_no, pub_date):
    """
    ç”Ÿæˆå¹¶è¿”å›žæ¶ˆæ¯çš„æ­£æ–‡éƒ¨åˆ†ï¼ŒåŒæ—¶ç”Ÿæˆæ‘˜è¦æ–‡ä»¶
    :param file_path: å‘¨åˆŠæ–‡ä»¶è·¯å¾„
    :param weekly_no: æœŸå·
    :param pub_date: å‘å¸ƒæ—¥æœŸ
    :return: æ ¼å¼åŒ–çš„æ­£æ–‡å†…å®¹
    """
    content_meta = get_front_matter(file_path)
    content_body = content_to_string(read_md(file_path))
    # åœ¨éžåœ¨çº¿çŽ¯å¢ƒä¸‹ç”Ÿæˆæ‘˜è¦æ–‡ä»¶
    online_action = os.getenv('ONLINE_ACTION')
    if not online_action:
        write_to_md_file(weekly_no, content_meta, content_body, pub_date, file_path)
    return content_body

def set_footer():
    """
    è¿”å›žTelegramæ¶ˆæ¯çš„é¡µè„šéƒ¨åˆ†
    :return: è®¢é˜…ä¿¡æ¯æ–‡æœ¬
    """
    return FOOTER_SUBSCRIPTION

def set_channel():
    """
    è¿”å›žTelegramé¢‘é“ä¿¡æ¯
    :return: é¢‘é“ä¿¡æ¯æ–‡æœ¬
    """
    return "ðŸ±é¢‘é“ @pythontrendingweekly"

async def send_to_telegram(bot_token, chat_id, text, image_path=None):
    """
    å‘é€æ¶ˆæ¯åˆ°Telegramé¢‘é“
    :param bot_token: Telegramæœºå™¨äººtoken
    :param chat_id: ç›®æ ‡é¢‘é“ID
    :param text: æ¶ˆæ¯æ–‡æœ¬
    :param image_path: å¯é€‰çš„å›¾ç‰‡è·¯å¾„
    """
    print("Sending content to tg bot")
    bot = Bot(token=bot_token)
    if image_path:
        with open(image_path, 'rb') as f:
            await bot.send_photo(chat_id=chat_id, photo=InputFile(f), caption=text, parse_mode='Markdown')
    else:
        await bot.send_message(chat_id=chat_id, text=text, parse_mode='Markdown', disable_web_page_preview=True)

def extract_weekly_no(file_path):
    """
    ä»Žæ–‡ä»¶å†…å®¹ä¸­æå–æœŸå·
    :param file_path: å‘¨åˆŠæ–‡ä»¶è·¯å¾„
    :return: æœŸå·å­—ç¬¦ä¸²
    """
    print(f"Extracting weekly number from {file_path}")
    with open(file_path, 'r', encoding="utf-8") as f:
        lines = f.readlines()
        match = re.search(r'#(\d+)', lines[2])
        if match:
            return match.group(1)
        else:
            raise ValueError("Invalid weekly no format in the third line.")

def get_message(weekly_no, content_body):
    """
    ç»„è£…å®Œæ•´çš„Telegramæ¶ˆæ¯
    :param weekly_no: æœŸå·
    :param content_body: æ­£æ–‡å†…å®¹
    :return: å®Œæ•´çš„æ¶ˆæ¯æ–‡æœ¬
    """
    print("Getting weekly message")
    header = set_title(weekly_no)
    footer = set_footer()
    channel = set_channel()
    return header + content_body + footer + channel

def count_words(file_path):
    """
    ç»Ÿè®¡markdownæ–‡ä»¶çš„å­—æ•°
    :param file_path: æ–‡ä»¶è·¯å¾„
    :return: å­—æ•°ç»Ÿè®¡
    æ³¨ï¼šä¸åŒ…æ‹¬é“¾æŽ¥URLå’Œå…ƒæ•°æ®éƒ¨åˆ†
    """
    print("Counting words in the file...")
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ç§»é™¤markdownçš„å…ƒæ•°æ®
    content = re.sub(r'---.*?---', '', content, flags=re.DOTALL)
    
    # ä¿ç•™é“¾æŽ¥æ–‡æœ¬ï¼Œç§»é™¤URL
    content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)
    
    # ç§»é™¤å…¶ä»–markdownæ ‡è®°
    content = re.sub(r'[#*`>]', '', content)  # ç§»é™¤#ã€*ã€`ã€>ç­‰markdownæ ‡è®°
    content = re.sub(r'\n+', '\n', content)   # å°†å¤šä¸ªæ¢è¡Œç¬¦æ›¿æ¢ä¸ºå•ä¸ª
    
    # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
    words = jieba.lcut(content)
    # è¿‡æ»¤æŽ‰ç©ºç™½å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·
    words = [word for word in words if word.strip()]
    
    return len(words)

def update_word_count(file_path, word_count):
    """
    æ›´æ–°æ–‡ä»¶ä¸­çš„å­—æ•°ç»Ÿè®¡
    :param file_path: æ–‡ä»¶è·¯å¾„
    :param word_count: å­—æ•°ç»Ÿè®¡ç»“æžœ
    """
    print("Updating word count in the file...")
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢å­—æ•°ï¼ŒåŒæ—¶å¤„ç†æœ‰æ•°å­—å’Œæ— æ•°å­—çš„æƒ…å†µ
    updated_content = re.sub(r'å…¨æ–‡(\s*?)(\d+)?(\s*?)å­—', f'å…¨æ–‡ {word_count} å­—', content)
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(updated_content)

def copy_to_archive(source_file, pub_date, weekly_no):
    """
    å¤åˆ¶ä¸­æ–‡å®Œæ•´ç‰ˆåˆ°ebookå½’æ¡£ç›®å½•
    :param source_file: æºæ–‡ä»¶è·¯å¾„
    :param pub_date: å‘å¸ƒæ—¥æœŸ
    :param weekly_no: æœŸå·
    """
    ebook_dir = os.path.expanduser('~/Documents/å‘¨åˆŠ/ebook/season4')
    if not os.path.exists(ebook_dir):
        os.makedirs(ebook_dir)
    ebook_target = os.path.join(ebook_dir, f'{pub_date}-weekly.md')
    print(f"Copying Chinese version to ebook directory: {ebook_target}")
    shutil.copy2(source_file, ebook_target)

def update_readme(weekly_file, weekly_no):
    """
    æ›´æ–°README.mdæ–‡ä»¶ï¼Œåœ¨å¾€æœŸåˆ—è¡¨éƒ¨åˆ†æ·»åŠ æ–°çš„å‘¨åˆŠé“¾æŽ¥
    :param weekly_file: å‘¨åˆŠæ–‡ä»¶è·¯å¾„
    :param weekly_no: æœŸå·
    """
    print("Updating README.md...")
    
    # è¯»å–å‘¨åˆŠæ–‡ä»¶çš„å…ƒæ•°æ®
    content_meta = get_front_matter(weekly_file)
    if not content_meta:
        print("Warning: Could not find front matter in weekly file")
        return
    
    # ä»Žtitleä¸­æå–å®žé™…æ ‡é¢˜ï¼ˆå†’å·åŽçš„å†…å®¹ï¼‰
    title = content_meta['title'].split('ï¼š', 1)[1] if 'ï¼š' in content_meta['title'] else content_meta['title']
        
    # ç”Ÿæˆæ–°çš„å‘¨åˆŠæ¡ç›®ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
    new_entry = f"- ç¬¬ {weekly_no} æœŸï¼š[{title}](./{weekly_file})\n"
    new_entry += f"  - {content_meta['description']}\n"
    
    # è¯»å–README.md
    with open('README.md', 'r', encoding='utf-8') as f:
        readme_content = f.read()
    
    # åœ¨"å¾€æœŸåˆ—è¡¨"éƒ¨åˆ†æ’å…¥æ–°æ¡ç›®
    section_start = "## ðŸ¦„å¾€æœŸåˆ—è¡¨\n\n"
    if section_start in readme_content:
        parts = readme_content.split(section_start)
        # åœ¨å¾€æœŸåˆ—è¡¨çš„å¼€å¤´æ’å…¥æ–°æ¡ç›®
        updated_content = parts[0] + section_start + new_entry + parts[1]
        
        # å†™å…¥æ›´æ–°åŽçš„å†…å®¹
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(updated_content)
            print("README.md updated successfully!")
    else:
        print("Warning: Could not find 'å¾€æœŸåˆ—è¡¨' section in README.md")

def process_weekly(pub_date=None):
    """
    å¤„ç†å‘¨åˆŠçš„ä¸»å‡½æ•°
    :param pub_date: å¯é€‰çš„å‘å¸ƒæ—¥æœŸï¼Œé»˜è®¤ä¸ºå½“å¤©
    å¤„ç†æµç¨‹ï¼š
    1. æ›´æ–°README.mdæ–‡ä»¶ï¼ˆä½¿ç”¨åŽŸå§‹çš„ä¸­è‹±å®Œæ•´ç‰ˆï¼‰
    2. æ‹†åˆ†ä¸­è‹±æ–‡ç‰ˆæœ¬ï¼ˆè‹±æ–‡ç‰ˆè‡ªåŠ¨ä¿å­˜åˆ°tmpç›®å½•ï¼Œä¸­æ–‡ç‰ˆè¦†ç›–åŽŸæ–‡ä»¶ï¼‰
    3. ç»Ÿè®¡ä¸­æ–‡ç‰ˆå­—æ•°å¹¶æ›´æ–°
    4. å¤åˆ¶ä¸­æ–‡ç‰ˆåˆ°ebookå½’æ¡£ç›®å½•
    5. ç”Ÿæˆä¸¤ä¸ªç‰ˆæœ¬çš„æ‘˜è¦æ–‡ä»¶ï¼ˆåšå®¢ç‰ˆå’ŒGithubç‰ˆï¼‰
    6. å‘é€æ¶ˆæ¯ç‰ˆåˆ°Telegram
    """
    if pub_date is None:
        pub_date = datetime.datetime.now().strftime('%Y-%m-%d')
    
    weekly_file = f'docs/{pub_date}-weekly.md'
    tmp_en_file = f'docs/en/tmp/{pub_date}-weekly.md'
    
    if not os.path.exists(weekly_file):
        print(f"File {weekly_file} does not exist.")
        sys.exit(1)
    
    # 1. æ›´æ–°README
    print("1. Updating README.md...")
    weekly_no = extract_weekly_no(weekly_file)
    update_readme(weekly_file, weekly_no)
    
    # 2. æ‹†åˆ†ä¸­è‹±æ–‡ç‰ˆæœ¬
    print("2. Splitting Chinese and English versions...")
    split_and_generate_files(weekly_file, tmp_en_file)
    
    # 3. ç»Ÿè®¡ä¸­æ–‡ç‰ˆå­—æ•°å¹¶æ›´æ–°
    print("3. Counting words and updating...")
    word_count = count_words(weekly_file)
    update_word_count(weekly_file, word_count)
    
    # 4. å¤åˆ¶å®Œæ•´ç‰ˆæ–‡ä»¶åˆ°å½’æ¡£ç›®å½•
    print("4. Copying files to archive...")
    copy_to_archive(weekly_file, pub_date, weekly_no)
    
    # 5. ç”Ÿæˆå‘¨åˆŠæ‘˜è¦ç‰ˆæœ¬
    content_meta = get_front_matter(weekly_file)
    content_body = content_to_string(read_md(weekly_file))
    write_to_md_file(weekly_no, content_meta, content_body, pub_date, weekly_file)
    
    # 6. ç”Ÿæˆæ‘˜è¦æ¶ˆæ¯å¹¶å‘é€åˆ°Telegram
    print("6. Generating summary files and sending to Telegram...")
    message = get_message(weekly_no, content_body)
    tg_bot_token = os.environ['TG_BOT_TOKEN']
    tg_chat_id = os.environ['TG_CHAT_ID']
    image_path = "resources/img/python-weekly.jpg"
    asyncio.run(send_to_telegram(tg_bot_token, tg_chat_id, message, image_path))
    
    print("Weekly processing completed!")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        process_weekly(sys.argv[1])
    else:
        process_weekly() 